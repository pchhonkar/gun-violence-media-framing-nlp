{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HW5 NLP Pipeline - End-to-End Analysis\n",
        "\n",
        "This notebook walks through the complete pipeline for analyzing entity descriptions across news sources.\n",
        "\n",
        "## Pipeline Overview\n",
        "1. Load articles from all news sources\n",
        "2. Resolve coreferences and extract entity contexts\n",
        "3. Extract descriptions (appositives, predicates, modifiers)\n",
        "4. Embed descriptions and cluster\n",
        "5. Generate frequency tables\n",
        "6. Run statistical tests\n",
        "7. Visualize results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add src to path\n",
        "import sys\n",
        "sys.path.insert(0, '..')\n",
        "\n",
        "# Standard imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Project imports\n",
        "from src import config\n",
        "from src import utils_io\n",
        "from src import load_articles\n",
        "from src import coref_contexts\n",
        "from src import extract_descriptions\n",
        "from src import embed_cluster\n",
        "from src import manual_eval_helpers\n",
        "from src import freq_tables\n",
        "from src import stats_tests\n",
        "\n",
        "# Ensure output directories exist\n",
        "config.ensure_output_dirs()\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(config.RANDOM_SEED)\n",
        "\n",
        "print(f\"Data directory: {config.DATA_DIR}\")\n",
        "print(f\"Output directory: {config.OUT_DIR}\")\n",
        "print(f\"News sources: {list(config.NEWS_SOURCES.keys())}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Articles\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load all articles\n",
        "# articles_df = load_articles.load_all_articles()\n",
        "# print(f\"Loaded {len(articles_df)} articles\")\n",
        "# articles_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Article statistics\n",
        "# stats = load_articles.get_article_stats(articles_df)\n",
        "# stats\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Coreference Resolution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load coreference model\n",
        "# coref_nlp = coref_contexts.load_coref_model()\n",
        "# print(\"Coreference model loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process articles for target entities\n",
        "# entity_contexts = coref_contexts.process_articles_for_coref(\n",
        "#     articles_df, \n",
        "#     target_entities=config.TARGET_ENTITIES\n",
        "# )\n",
        "# print(f\"Extracted contexts for {len(entity_contexts)} entities\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Extract Descriptions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load spaCy model for description extraction\n",
        "# nlp = extract_descriptions.load_spacy_model()\n",
        "# print(f\"Loaded spaCy model: {config.SPACY_MODEL}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract descriptions from contexts\n",
        "# descriptions = extract_descriptions.process_contexts_for_descriptions(\n",
        "#     entity_contexts,\n",
        "#     nlp\n",
        "# )\n",
        "# print(f\"Extracted descriptions for {len(descriptions)} entities\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Embedding & Clustering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load embedding model\n",
        "# embed_model = embed_cluster.load_embedding_model()\n",
        "# print(f\"Loaded embedding model: {config.EMBEDDING_MODEL}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run embedding and clustering pipeline\n",
        "# For each target entity:\n",
        "# results = {}\n",
        "# for entity in config.TARGET_ENTITIES:\n",
        "#     results[entity] = embed_cluster.run_embedding_clustering_pipeline(\n",
        "#         descriptions[entity],\n",
        "#         entity_name=entity\n",
        "#     )\n",
        "#     print(f\"Processed {entity}: {len(results[entity]['labels'])} descriptions\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize clusters (UMAP)\n",
        "# TODO: Add UMAP visualization code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Frequency Tables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate frequency tables\n",
        "# freq_table = freq_tables.compute_cluster_frequencies(\n",
        "#     results['Trump']['labels'],\n",
        "#     results['Trump']['sources']\n",
        "# )\n",
        "# freq_table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalized frequencies\n",
        "# norm_freq = freq_tables.compute_normalized_frequencies(freq_table)\n",
        "# norm_freq\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Statistical Tests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chi-square test\n",
        "# contingency = freq_tables.create_contingency_table(\n",
        "#     results['Trump']['labels'],\n",
        "#     results['Trump']['sources']\n",
        "# )\n",
        "# chi2_results = stats_tests.chi_square_test(contingency)\n",
        "# print(f\"Chi-square statistic: {chi2_results['chi2']:.2f}\")\n",
        "# print(f\"P-value: {chi2_results['p_value']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Effect size\n",
        "# effect = stats_tests.compute_effect_size(contingency)\n",
        "# print(f\"Cram√©r's V: {effect:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pairwise comparisons\n",
        "# pairwise = stats_tests.pairwise_source_comparisons(\n",
        "#     results['Trump']['labels'],\n",
        "#     results['Trump']['sources']\n",
        "# )\n",
        "# pairwise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Manual Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create evaluation sample\n",
        "# eval_sample = manual_eval_helpers.create_evaluation_sample(\n",
        "#     results['Trump']['descriptions'],\n",
        "#     results['Trump']['labels'],\n",
        "#     n_per_cluster=5\n",
        "# )\n",
        "# eval_sample.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export for annotation\n",
        "# manual_eval_helpers.export_for_annotation(\n",
        "#     eval_sample,\n",
        "#     config.REPORTS_DIR / 'evaluation_sample.csv'\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save all results\n",
        "# utils_io.save_pickle(results, config.PROCESSED_DIR / 'clustering_results.pkl')\n",
        "# freq_tables.export_frequency_tables({'cluster_freq': freq_table}, config.REPORTS_DIR)\n",
        "# stats_tests.export_stats_report(chi2_results, config.REPORTS_DIR / 'stats_report.txt')\n",
        "# print(\"Results saved!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "TODO: Add summary of findings after running the analysis.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
